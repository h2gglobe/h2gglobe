#! /usr/bin/python
import sys, os, re
import datetime, time
import shlex as _shlex
import subprocess as _subprocess

def source(script, update=1):
    p = _subprocess.Popen(". %s; env" % script, stdout=_subprocess.PIPE, shell=True)
    data = p.communicate()[0]

    env=dict()
    #env = dict((line.split("=", 1) for line in data.splitlines()))
    for line in data.splitlines():
        param = line.split("=", 1)
        if (len(param) > 2):
            env[param[0]]= param[1]
        
    if update:
        os.environ.update(env)

def pipe(cmdline, input = None):
  """
  wrapper around subprocess to simplify te interface
  """
  args = _shlex.split(cmdline)

  if input is not None:
    command = _subprocess.Popen(args, stdin = _subprocess.PIPE, stdout = _subprocess.PIPE, stderr = None)
  else:
    command = _subprocess.Popen(args, stdin = None, stdout = _subprocess.PIPE, stderr = None)

  (out, err) = command.communicate(input)
  return out

if __name__ == "__main__":

    # check host
    if ("uaf" not in os.getenv("HOSTNAME")):
        print "Sorry but the script is done to be run on UAF machines at UCSD."
        sys.exit(-1)
    
    top = os.getcwd()
    temp = ""
    temp1 = pipe("scram tool info root", temp)
    temp2 = pipe("grep Version", temp1)
    rootVersion = pipe("awk '{print $3}'", temp2).split("\n")[0]
    CMSSWVersion  = re.search("(CMSSW.+)/s", top).group(1)
    username      = os.getlogin()
    hadoop_prefix = "srm://bsrm-1.t2.ucsd.edu:8443/srm/v2/server?SFN="
    hadoop_home   = "/hadoop/cms/store/user/" + username

    # reduce_batch input ---
    REQUIRED_ARGS = 9
    
    if (len(sys.argv) < REQUIRED_ARGS):
        print "input ${INPUT_ARGS} arguments, ${REQUIRED_ARGS} required -- aborting\n"
        print "Usage: is ./reduce_batch arg1 arg2 arg3 arg4 arg5 arg6 arg7 arg8\n"
        print "arg1: tag \t\t\t\t\t(jets, hww160, ttbar, ...)"
        print "arg2: analysisname \t\t\t\t(PhotonAnalysis, ...)"
        print "arg3: type \t\t\t\t(for reducing differently for different data)"
        print "arg4: root_seed \t\t\t(the beginning of your filenames to run on)"
        print "arg5: hadoop_data_directory \t\t( /hadoop/cms/store/user/ . . . )"
        print "arg6: dat file (relative to AnalysisScripts)\t\t\t"
        print "arg7: inputBranches file (relative to AnalysisScripts)\t\t\t"
        print "arg8: outputBranches file (relative to AnalysisScripts)\t\t\t"
        sys.exit(-4)
    print "\nPreparing batch reduction...\n"
    
    tag = sys.argv[1]
    analysisname = sys.argv[2]
    itype = sys.argv[3]
    rootseed = sys.argv[4]
    thedatadirectory = os.path.dirname(sys.argv[5]) + "/"+ os.path.basename(sys.argv[5])
    datfile = sys.argv[6]
    inputBranchesFile = sys.argv[7]
    outputBranchesFile = sys.argv[8]
    numberofjobs = 0
    if (len(sys.argv) == 10):
        numberofjobs = sys.argv[9]

    basedir = os.path.abspath(sys.argv[0].split("Matteo/reduce_batch")[0])
    basename = os.path.basename(basedir)
    os.system("cd "+basedir)
    
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    outputdir = timestamp + "_" + tag
    #theglobedirectory = os.path.dirname(os.getcwd())
    #print theglobedirectory
    #sys.exit()
    os.system("rm -rf reduce")
    os.system("mkdir reduce")
    
    #**********************************************************************
    os.system("ls -1 "+ thedatadirectory + "/" + rootseed + "*.root | grep -v _CMSSW_ > reduce/allfilenames.txt")
    
    #**********************************************************************
    totalfiles = pipe("wc -l reduce/allfilenames.txt")
    totalfiles = int(totalfiles.split(" ")[0])
    if (totalfiles == 0):
        print "no files ??"
        sys.exit(-5)
    
    #*************************************************************************************
    totalsizelimit = 10000000000
    
    a,b,c = os.popen3("ls -l " + thedatadirectory + "/" + rootseed + "*.root")
    temp = ""
    for i in b.readlines():
        temp += i
    totalfilesize = int(pipe("awk \'{sum+=$5}; END{print sum}\'", temp))

    if (numberofjobs > totalfiles):
        numberofjobs = totalfiles
        
    if (numberofjobs == 0):
        numberofjobs = totalfilesize/totalsizelimit + 1
        
    avgfilesize  = totalfilesize/totalfiles
    filesperjob  = totalfiles/numberofjobs + 1
    if (filesperjob > 500):
        print "\ntoo many files per job: " + str(filesperjob) + ".  Limiting to 500."
        filesperjob=500
        numberofjobs = totalfiles/500 + 1
    
    filesinlastjob = totalfiles - filesperjob *(numberofjobs - 1)
    if ((filesinlastjob < 0) == 1):
        numberofjobs   = numberofjobs + filesinlastjob/filesperjob - 1
        filesinlastjob = totalfiles - filesperjob*(numberofjobs - 1)
    
    tenG = 10000000000
    tenM = 10000000
    tenk = 10000
    
    totalfilesizeprint = totalfilesize
    if (totalfilesizeprint > tenG):
        totalfilesizeprint = totalfilesizeprint/(tenG/10)
        totalfilesizeprint = str(totalfilesizeprint) + " G"
    else:
      if (totalfilesizeprint > tenM):
          totalfilesizeprint = totalfilesizeprint/(tenM/10)
          totalfilesizeprint = str(totalfilesizeprint) + " M"
      else:
        if (totalfilesizeprint > tenk):
            totalfilesizeprint = totalfilesizeprint/(tenk/10)
            totalfilesizeprint = str(totalfilesizeprint) + " k"
    
    avgfilesizeprint = avgfilesize
    if (avgfilesizeprint > tenG):
        avgfilesizeprint = avgfilesizeprint/(tenG/10)
        avgfilesizeprint = str(avgfilesizeprint) + " G"
    else:
      if (avgfilesizeprint > tenM):
          avgfilesizeprint = avgfilesizeprint/(tenM/10)
          avgfilesizeprint = str(avgfilesizeprint) + " M"
      else:
        if (avgfilesizeprint > tenk):
            avgfilesizeprint = avgfilesizeprint/(tenk/10)
            avgfilesizeprint = str(avgfilesizeprint) + " k"
    
    print "\ntotalfiles:     ", totalfiles
    print "totalfilesize:  ", totalfilesizeprint
    print "avgfilesize:    ", avgfilesizeprint
    print "numberofjobs:   ", numberofjobs
    print "filesperjob:    ", filesperjob
    print "filesinlastjob: ", filesinlastjob,"\n"
    print "reducing " + str(totalfiles) +" files in " + str(numberofjobs) + " jobs (avg file size: "+ str(avgfilesize) +")."
    
    #*************************************************************************************
    source("/code/osgcode/ucsdt2/gLite32/etc/profile.d/grid_env.sh")
    os.environ["LCG_GFAL_INFOSYS"] = "lcg-bdii.cern.ch:2170"
    os.environ["GLOBUS_TCP_PORT_RANGE"] = "20000,25000"
    source("/code/osgcode/ucsdt2/Crab/etc/crab.sh")

    a,b,c = os.popen3("voms-proxy-info 2>&1")
    temp = ""
    for i in b.readlines():
        temp += i
    temp1 = pipe("grep time", temp)
    temp2 = pipe("awk -F: \'{if($2 > 30) print \"proxy ok\"}\'", temp1)
    temp3 = pipe("grep \"proxy ok\"", temp2)
    if (temp3 != "proxy ok\n"):
        os.system("voms-proxy-destroy")
        os.system("voms-proxy-init -valid 36:00")
    
    file = open("reduce/allfilenames.txt")
    file_names = file.readlines()
    file.close()
    
    outputfilename = rootseed + "_red_" + tag + "_typ" + str(itype)
    numberofjobs = 0
    
    for f in xrange(0, len(file_names), filesperjob):
        numberofjobs += 1
        limits = (f, filesperjob+f)
        if (limits[1] > len(file_names)):
            limits = (f, len(file_names))
        
        #inp = open(analysisname+"_scripts/"+templatefile, "r")
        #lines = inp.readlines()
        #inp.close()
    
        datafilename = rootseed + "_datafiles"+ str(numberofjobs) + ".dat"
        txtfilename = rootseed + "_datafiles"+ str(numberofjobs) + ".txt"
        
        out = open("reduce/"+datafilename, "w")
        out2 = open("reduce/"+txtfilename, "w")
        out.write("output=" + outputfilename + "_job" + str(numberofjobs) +".root\n")
        for i in file_names[limits[0]:limits[1]]:
            out.write("Fil=" + i.split("\n")[0] + " typ=" + itype + "\n")
            out2.write(i)
            out.write("\n")    
        out.write("inputBranches " + inputBranchesFile + "\n")
        out.write("outputBranches " + outputBranchesFile + "\n")
        out.write("analyzer " + analysisname + " " + datfile + "\n")
                
        out.close()
        out2.close()
    
    os.system("cp -r AnalysisScripts/* reduce/.")
    
    thetarball = "reduce" + timestamp + ".tgz"
    os.system("cd ../..;tar zcf " + thetarball + " src/"+basename+"/reduce src/"+basename+"/libLoopAll.so  src/EGamma src/CMGTools")
    
    if (not os.path.exists(hadoop_home + "/reduced")):
        os.mkdir(hadoop_home + "/reduced")
    
    if (not os.path.exists(hadoop_home + "/condorfiles")):
        os.mkdir(hadoop_home + "/condorfiles")
    
    os.system("cp ../../" + thetarball + " " + hadoop_home + "/condorfiles/")
    os.system("rm ../../" + thetarball)
    
    for ijob in xrange(1, numberofjobs):
        datafilename = rootseed + "_datafiles" + str(ijob) + ".dat"
        os.system("rm -f " + datafilename)
    
    os.system("rm -f reduce/allfilenames.txt")
    condordir = "/home/users/" + username + "/condorfiles/" + timestamp + "_red_" + rootseed
    
    os.mkdir(hadoop_home + "/reduced/" + outputdir)
    
    if (not os.path.exists("/home/users/" + username + "/condorfiles")):
        os.mkdir("/home/users/" + username + "/condorfiles")
        
    if (not os.path.exists(condordir)):
        os.mkdir(condordir)
    
    os.chdir(condordir)
    os.mkdir("output")
    
    for ijob in xrange(1, numberofjobs+1):
        jobname = "reduce_"+ rootseed + "_job" + str(ijob)
        thesourcedir = username + timestamp + "job" + str(ijob)
        datafilename = rootseed + "_datafiles" + str(ijob) + ".dat"
        txtfilename = rootseed + "_datafiles"+ str(numberofjobs) + ".txt"
           
        file = open(jobname + ".bash", "w")
        file.write("#!/bin/bash\n")
        file.write("umask 002\n")
        file.write("\n")
        file.write("## setup env\n")
        file.write("export PATH=/usr/local/bin:/bin:/usr/bin\n")
        file.write("export SCRAM_ARCH=slc5_amd64_gcc462\n")
        file.write("source $OSG_APP/cmssoft/cms/cmsset_default.sh\n")
        file.write("scram project CMSSW " + CMSSWVersion + "\n")
        file.write("cd " + CMSSWVersion + "/src\n")
        file.write("eval \`scram runtime -sh\`\n")
        file.write("export LD_LIBRARY_PATH=${LD_LIBRARY_PATH#\\\"}\n")
        file.write("export LD_LIBRARY_PATH=${LD_LIBRARY_PATH%\\\";}\n")
        file.write("export PYTHONPATH=${PYTHONPATH#\\\"}\n")
        file.write("export PYTHONPATH=${PYTHONPATH%\\\";}\n")
        file.write("export CMSSW_SEARCH_PATH=${CMSSW_SEARCH_PATH#\\\"}\n")
        file.write("export CMSSW_SEARCH_PATH=${CMSSW_SEARCH_PATH%\\\";}\n")
        file.write("export CMSSW_BASE=${CMSSW_BASE#\\\"}\n")
        file.write("export CMSSW_BASE=${CMSSW_BASE%\\\";}\n")
        file.write("export CMSSW_SEARCH_PATH=$CMSSW_SEARCH_PATH:$CMSSW_BASE/CMGTools/External/data\n")
        file.write("export PATH=${PATH#\\\"}\n")
        file.write("export PATH=${PATH%\\\";}\n")
        file.write("export SRM_PATH=$OSG_APP/ucsdt2/gLite/d-cache/srm:$OSG_APP/ucsdt2/gLite32/d-cache/srm\n")
        file.write("export PATH=$PATH:$OSG_APP/ucsdt2/gLite/d-cache/srm/bin:$OSG_APP/ucsdt2/gLite32/d-cache/srm/bin\n")
        file.write("export JAVA_HOME=/usr/java/latest\n")
        file.write("landdir=\"$PWD\"\n")
        file.write("sourcedir=\"$PWD/%s\"\n" %(thesourcedir))
        file.write("\n")
        file.write("cd ..\n")
        file.write("cp %s/condorfiles/%s .\n" % (hadoop_home, thetarball))

        file.write("hostname\n")
        file.write("whereis srmcp\n")
        file.write("which srmcp\n")
        file.write("ls $OSG_APP/ucsdt2/gLite/d-cache/srm\n")
        
        file.write("tar -zxvf %s\n" % (thetarball))
        file.write("rm %s\n" % (thetarball) )
        file.write("cd src\n")
        file.write("scram b -j 6\n")
        file.write("cd "+basename+"/reduce\n")
        file.write("mkdir ${sourcedir}\n")
        file.write("\n")
    
        file.write("./reduce.py -i %s\n" % (datafilename))
        #file.write("echo \"landir\"\n")
        #file.write("echo ${landdir}\n")
        file.write("\n")
        file.write("ls -1 ${landdir}/"+basename+"/reduce/*.root > theoutputfiles.txt\n")
    
        file.write("\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("cat theoutputfiles.txt\n")
        file.write("echo \"****************** catting theoutputfiles.txt ****************\"\n")
        file.write("\n")
    
        file.write("for ifile in `/usr/bin/less theoutputfiles.txt`;do\n")
        file.write("barefilename=`basename ${ifile}`\n")
        file.write("\n")
        file.write("srmcp -2 -debug=true -delegate=false file:////${ifile} %s%s/reduced/%s/${barefilename} > srmcpfile.txt 2>&1 \n" % (hadoop_prefix, hadoop_home, outputdir))
        file.write("/usr/bin/less srmcpfile.txt\n")
        file.write("\n")
        file.write("if [ ! `grep -i fail srmcpfile.txt  | awk '{print NR}'| /usr/bin/tail -1` ];then \n")
        file.write("    echo \"DONE\"\n")
        file.write("else\n")
        file.write("   echo \"srmcp failed - attempting globus-url-copy to home area of %s\"; \n" % (username))
        file.write("   globus-url-copy file:///${ifile} gsiftp://uaf-2.t2.ucsd.edu/home/users/%s/${barefilename} > globusurlcopy.txt\n" % (username))
        file.write("   if [ ! `less globusurlcopy.txt | awk '{print NR}'|tail -1` ]; then\n")
        file.write("      echo \"DONE\"\n")
        file.write("   else\n")
        file.write("     echo \"globus-url-copy failed also - losing output, sorry - updating database with status 0\"; \n")
        file.write("   fi\n")
        file.write("fi\n")
        file.write("\n")
        file.write("\n")
        file.write("done\n")
        file.write("\n")
        file.write("cd ../../../..\n")
        file.write("echo \"clean up -------------------------------------\"\n")
        file.write("rm -rf " + CMSSWVersion + "\n")
        file.write("exit\n")
        file.close()
        
        file = open(jobname + ".sub", "w")
        text = """
    universe=grid
    Grid_Resource=gt2 osg-gw-4.t2.ucsd.edu:/jobmanager-condor
    executable=%s.bash
    stream_output = False
    stream_error  = False
    WhenToTransferOutput = ON_EXIT
    transfer_input_files =
    transfer_Output_files =
    log    = %s/test%s.log
    Notification = Never
    
    
    output = ./output/condor_batch.$(Cluster).$(Process).out
    error  = ./output/condor_batch.$(Cluster).$(Process).err
    queue
    """ % (jobname, condordir, str(ijob))
        file.write(text)
        file.close()
        
        print "\nsubmitting " + jobname + ".sub\n"
        os.system("condor_submit " + jobname + ".sub")
        time.sleep(1)
    
